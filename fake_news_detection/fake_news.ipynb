{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8228d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f331f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Regional African body says ready to work close...</td>\n",
       "      <td>JOHANNESBURG (Reuters) - The Southern African ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>New Report Says Trump Destroyed Scaramucci’s ...</td>\n",
       "      <td>Donald Trump s new pick for Communications Dir...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hamas chief in Gaza says Palestinian unity dea...</td>\n",
       "      <td>GAZA (Reuters) - Palestinian Islamist group Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Police fire tear gas at Congo opposition leade...</td>\n",
       "      <td>KINSHASA (Reuters) - Police fired tear gas to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PRESIDENT TRUMP Hits Back At Activist Judge On...</td>\n",
       "      <td>Judge Orrick in California ruled against Presi...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1  Regional African body says ready to work close...   \n",
       "1   2   New Report Says Trump Destroyed Scaramucci’s ...   \n",
       "2   3  Hamas chief in Gaza says Palestinian unity dea...   \n",
       "3   4  Police fire tear gas at Congo opposition leade...   \n",
       "4   5  PRESIDENT TRUMP Hits Back At Activist Judge On...   \n",
       "\n",
       "                                                text label Unnamed: 4  \\\n",
       "0  JOHANNESBURG (Reuters) - The Southern African ...     1        NaN   \n",
       "1  Donald Trump s new pick for Communications Dir...     0        NaN   \n",
       "2  GAZA (Reuters) - Palestinian Islamist group Ha...     1        NaN   \n",
       "3  KINSHASA (Reuters) - Police fired tear gas to ...     1        NaN   \n",
       "4  Judge Orrick in California ruled against Presi...     0        NaN   \n",
       "\n",
       "  Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9  ... Unnamed: 12  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "1        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "2        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "3        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "4        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "\n",
       "  Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 19 Unnamed: 20 Unnamed: 21  \n",
       "0         NaN         NaN         NaN  \n",
       "1         NaN         NaN         NaN  \n",
       "2         NaN         NaN         NaN  \n",
       "3         NaN         NaN         NaN  \n",
       "4         NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52466c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Regional African body says ready to work close...</td>\n",
       "      <td>JOHANNESBURG (Reuters) - The Southern African ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>New Report Says Trump Destroyed Scaramucci’s ...</td>\n",
       "      <td>Donald Trump s new pick for Communications Dir...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hamas chief in Gaza says Palestinian unity dea...</td>\n",
       "      <td>GAZA (Reuters) - Palestinian Islamist group Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Police fire tear gas at Congo opposition leade...</td>\n",
       "      <td>KINSHASA (Reuters) - Police fired tear gas to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PRESIDENT TRUMP Hits Back At Activist Judge On...</td>\n",
       "      <td>Judge Orrick in California ruled against Presi...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1  Regional African body says ready to work close...   \n",
       "1   2   New Report Says Trump Destroyed Scaramucci’s ...   \n",
       "2   3  Hamas chief in Gaza says Palestinian unity dea...   \n",
       "3   4  Police fire tear gas at Congo opposition leade...   \n",
       "4   5  PRESIDENT TRUMP Hits Back At Activist Judge On...   \n",
       "\n",
       "                                                text label Unnamed: 4  \\\n",
       "0  JOHANNESBURG (Reuters) - The Southern African ...     1        NaN   \n",
       "1  Donald Trump s new pick for Communications Dir...     0        NaN   \n",
       "2  GAZA (Reuters) - Palestinian Islamist group Ha...     1        NaN   \n",
       "3  KINSHASA (Reuters) - Police fired tear gas to ...     1        NaN   \n",
       "4  Judge Orrick in California ruled against Presi...     0        NaN   \n",
       "\n",
       "  Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9  ... Unnamed: 12  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "1        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "2        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "3        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "4        NaN        NaN        NaN        NaN        NaN  ...         NaN   \n",
       "\n",
       "  Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 19 Unnamed: 20 Unnamed: 21  \n",
       "0         NaN         NaN         NaN  \n",
       "1         NaN         NaN         NaN  \n",
       "2         NaN         NaN         NaN  \n",
       "3         NaN         NaN         NaN  \n",
       "4         NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('train.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b650c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = train_df.select_dtypes(include=['object'])\n",
    "num_df = train_df.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6130f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24360 entries, 0 to 24359\n",
      "Data columns (total 22 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           24360 non-null  int64  \n",
      " 1   title        24360 non-null  object \n",
      " 2   text         24356 non-null  object \n",
      " 3   label        24348 non-null  object \n",
      " 4   Unnamed: 4   1 non-null      object \n",
      " 5   Unnamed: 5   1 non-null      object \n",
      " 6   Unnamed: 6   1 non-null      object \n",
      " 7   Unnamed: 7   1 non-null      object \n",
      " 8   Unnamed: 8   1 non-null      object \n",
      " 9   Unnamed: 9   1 non-null      object \n",
      " 10  Unnamed: 10  1 non-null      object \n",
      " 11  Unnamed: 11  1 non-null      object \n",
      " 12  Unnamed: 12  1 non-null      object \n",
      " 13  Unnamed: 13  1 non-null      object \n",
      " 14  Unnamed: 14  1 non-null      object \n",
      " 15  Unnamed: 15  1 non-null      object \n",
      " 16  Unnamed: 16  1 non-null      object \n",
      " 17  Unnamed: 17  1 non-null      object \n",
      " 18  Unnamed: 18  1 non-null      object \n",
      " 19  Unnamed: 19  1 non-null      object \n",
      " 20  Unnamed: 20  1 non-null      object \n",
      " 21  Unnamed: 21  1 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(20)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a50cefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unnamed: 4</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unnamed: 5</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Unnamed: 9</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Unnamed: 8</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Unnamed: 7</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Unnamed: 6</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Unnamed: 14</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Unnamed: 15</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unnamed: 16</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Unnamed: 17</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Unnamed: 10</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Unnamed: 11</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Unnamed: 12</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Unnamed: 13</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Unnamed: 18</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Unnamed: 19</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Unnamed: 20</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Unnamed: 21</td>\n",
       "      <td>99.995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>label</td>\n",
       "      <td>0.049261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>0.016420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    column_name  percentage\n",
       "4    Unnamed: 4   99.995895\n",
       "5    Unnamed: 5   99.995895\n",
       "9    Unnamed: 9   99.995895\n",
       "8    Unnamed: 8   99.995895\n",
       "7    Unnamed: 7   99.995895\n",
       "6    Unnamed: 6   99.995895\n",
       "14  Unnamed: 14   99.995895\n",
       "15  Unnamed: 15   99.995895\n",
       "16  Unnamed: 16   99.995895\n",
       "17  Unnamed: 17   99.995895\n",
       "10  Unnamed: 10   99.995895\n",
       "11  Unnamed: 11   99.995895\n",
       "12  Unnamed: 12   99.995895\n",
       "13  Unnamed: 13   99.995895\n",
       "18  Unnamed: 18   99.995895\n",
       "19  Unnamed: 19   99.995895\n",
       "20  Unnamed: 20   99.995895\n",
       "21  Unnamed: 21   99.995895\n",
       "3         label    0.049261\n",
       "2          text    0.016420"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_df = (train_df.isnull().mean() * 100).reset_index()\n",
    "nan_df.columns = ['column_name', 'percentage']\n",
    "nan_df.sort_values('percentage', ascending=False, inplace=True)\n",
    "nan_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5bdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = nan_df[nan_df['percentage'] > 90]['column_name']\n",
    "train_df = train_df.drop(columns=columns_to_drop)\n",
    "test_df = test_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd09baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = train_df.select_dtypes(include=['object'])\n",
    "num_df = train_df.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53dd418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24360 entries, 0 to 24359\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   24360 non-null  object\n",
      " 1   text    24356 non-null  object\n",
      " 2   label   24348 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 571.1+ KB\n"
     ]
    }
   ],
   "source": [
    "cat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbca4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_value = train_df['text'].mode()[0]\n",
    "train_df['text'] = train_df['text'].fillna(mode_value)\n",
    "test_df['text'] = test_df['text'].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652991a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].fillna(0)\n",
    "test_df['label'] = test_df['label'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22779ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24360 entries, 0 to 24359\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   id      24360 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 190.4 KB\n"
     ]
    }
   ],
   "source": [
    "num_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd3be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропуски в train_df: id       0\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Пропуски в test_df: id       0\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Пропуски в train_df:\", train_df.isnull().sum())\n",
    "print(\"Пропуски в test_df:\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "430bf1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Regional African body says ready to work close...</td>\n",
       "      <td>JOHANNESBURG (Reuters) - The Southern African ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>New Report Says Trump Destroyed Scaramucci’s ...</td>\n",
       "      <td>Donald Trump s new pick for Communications Dir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hamas chief in Gaza says Palestinian unity dea...</td>\n",
       "      <td>GAZA (Reuters) - Palestinian Islamist group Ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Police fire tear gas at Congo opposition leade...</td>\n",
       "      <td>KINSHASA (Reuters) - Police fired tear gas to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PRESIDENT TRUMP Hits Back At Activist Judge On...</td>\n",
       "      <td>Judge Orrick in California ruled against Presi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1  Regional African body says ready to work close...   \n",
       "1   2   New Report Says Trump Destroyed Scaramucci’s ...   \n",
       "2   3  Hamas chief in Gaza says Palestinian unity dea...   \n",
       "3   4  Police fire tear gas at Congo opposition leade...   \n",
       "4   5  PRESIDENT TRUMP Hits Back At Activist Judge On...   \n",
       "\n",
       "                                                text label  \n",
       "0  JOHANNESBURG (Reuters) - The Southern African ...     1  \n",
       "1  Donald Trump s new pick for Communications Dir...     0  \n",
       "2  GAZA (Reuters) - Palestinian Islamist group Ha...     1  \n",
       "3  KINSHASA (Reuters) - Police fired tear gas to ...     1  \n",
       "4  Judge Orrick in California ruled against Presi...     0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37359b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Предобработка данных\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m [train_df, test_df]:\n\u001b[1;32m---> 50\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     52\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\artificial_intelligence\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\artificial_intelligence\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\artificial_intelligence\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\artificial_intelligence\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\artificial_intelligence\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[59], line 31\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|https\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mMULTILINE)  \u001b[38;5;66;03m# Удаление URL\u001b[39;00m\n\u001b[0;32m     30\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Удаление пунктуации и цифр\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Инициализация\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fake_keywords = ['shock', 'urgent', 'breaking', 'secret', 'hoax', 'fake', 'scandal', 'conspiracy']\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Удаление URL\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Удаление пунктуации и цифр\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Функция очистки меток\n",
    "def clean_label(label):\n",
    "    if pd.isna(label):\n",
    "        return 0\n",
    "    try:\n",
    "        return int(float(label))\n",
    "    except (ValueError, TypeError):\n",
    "        label = str(label).strip().lower()\n",
    "        return 1 if label in ['1', 'true', 'real', 'yes'] else 0\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Предобработка данных\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['text'].fillna('').apply(preprocess_text)\n",
    "    df['title'] = df['title'].fillna('').apply(preprocess_text)\n",
    "    df['combined'] = df['title'] + ' ' + df['text']\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    df['title_length'] = df['title'].apply(len)\n",
    "    df['word_count'] = df['combined'].apply(lambda x: len(x.split()))\n",
    "    df['fake_keyword_count'] = df['combined'].apply(lambda x: sum(x.lower().count(kw) for kw in fake_keywords))\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(clean_label).astype('int8')\n",
    "\n",
    "# Разделение данных\n",
    "X = train_df[['combined', 'text_length', 'title_length', 'word_count', 'fake_keyword_count']]\n",
    "y = train_df['label']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Определение препроцессора\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_tfidf', TfidfVectorizer(max_features=15000, ngram_range=(1, 3), stop_words='english'), 'combined'),\n",
    "        ('num', 'passthrough', ['text_length', 'title_length', 'word_count', 'fake_keyword_count'])\n",
    "    ])\n",
    "\n",
    "# Создание пайплайна\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=sum(y_train == 0) / sum(y_train == 1)  # Балансировка классов\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Обучение модели\n",
    "print(\"Обучение модели...\")\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"Время обучения: {time.time() - start_time:.2f} сек\")\n",
    "\n",
    "# Оценка на валидационной выборке\n",
    "valid_pred = pipeline.predict(X_valid)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, valid_pred))\n",
    "print(f\"F1-score (Validation): {f1_score(y_valid, valid_pred, average='weighted'):.4f}\")\n",
    "\n",
    "# Анализ ошибок\n",
    "errors = X_valid[valid_pred != y_valid].copy()\n",
    "errors['true_label'] = y_valid[valid_pred != y_valid]\n",
    "errors['predicted_label'] = valid_pred[valid_pred != y_valid]\n",
    "print(\"\\nПримеры ошибок (первые 5):\")\n",
    "print(errors[['combined', 'true_label', 'predicted_label']].head())\n",
    "print(\"\\nСтатистика ошибок:\")\n",
    "print(f\"Всего ошибок: {len(errors)}\")\n",
    "print(f\"Ошибки для класса 0 (фейк): {len(errors[errors['true_label'] == 0])}\")\n",
    "print(f\"Ошибки для класса 1 (реальная): {len(errors[errors['true_label'] == 1])}\")\n",
    "\n",
    "# Предсказание на тестовых данных\n",
    "test_pred = pipeline.predict(test_df[['combined', 'text_length', 'title_length', 'word_count', 'fake_keyword_count']])\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "#results.to_csv('improved_predictions.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в improved_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e51fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обучения: 32.93 сек\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\artificial_intelligence\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      2249\n",
      "           1       0.99      0.98      0.99      2623\n",
      "\n",
      "    accuracy                           0.99      4872\n",
      "   macro avg       0.99      0.99      0.99      4872\n",
      "weighted avg       0.99      0.99      0.99      4872\n",
      "\n",
      "F1-score: 0.9854\n",
      "\n",
      "Предсказания сохранены в optimized_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\artificial_intelligence\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from time import time\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    return text\n",
    "\n",
    "# Загрузка данных с правильными типами\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv', dtype={'id': 'int32', 'title': 'string', 'text': 'string'})\n",
    "    test_df = pd.read_csv('test.csv', dtype={'id': 'int32', 'title': 'string', 'text': 'string'})\n",
    "except UnicodeDecodeError:\n",
    "    train_df = pd.read_csv('train.csv', dtype={'id': 'int32', 'title': 'string', 'text': 'string'}, encoding='latin1')\n",
    "    test_df = pd.read_csv('test.csv', dtype={'id': 'int32', 'title': 'string', 'text': 'string'}, encoding='latin1')\n",
    "\n",
    "# Предобработка меток\n",
    "def clean_label(label):\n",
    "    if pd.isna(label):\n",
    "        return 0\n",
    "    label = str(label).strip().lower()\n",
    "    if label in ['1', 'true', 'real', 'yes']:\n",
    "        return 1\n",
    "    elif label in ['0', 'false', 'fake', 'no']:\n",
    "        return 0\n",
    "    return int(float(label)) if label.replace('.', '').isdigit() else 0\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(clean_label).astype('int8')\n",
    "\n",
    "# Быстрая предобработка текста\n",
    "train_df['combined'] = (train_df['title'].fillna('') + ' ' + train_df['text'].fillna('')).apply(preprocess_text)\n",
    "test_df['combined'] = (test_df['title'].fillna('') + ' ' + test_df['text'].fillna('')).apply(preprocess_text)\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['combined'],\n",
    "    train_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Оптимизированный пайплайн\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('model', LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Обучение модели\n",
    "start_time = time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"Время обучения: {time() - start_time:.2f} сек\")\n",
    "\n",
    "# Оценка\n",
    "valid_pred = pipeline.predict(X_valid)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, valid_pred))\n",
    "print(f\"F1-score: {f1_score(y_valid, valid_pred, average='weighted'):.4f}\")\n",
    "\n",
    "# Предсказание\n",
    "test_pred = pipeline.predict(test_df['combined'])\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "#results.to_csv('optimized_predictions.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в optimized_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e3674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\artificial_intelligence\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      2249\n",
      "           1       0.99      0.98      0.99      2623\n",
      "\n",
      "    accuracy                           0.99      4872\n",
      "   macro avg       0.99      0.99      0.99      4872\n",
      "weighted avg       0.99      0.99      0.99      4872\n",
      "\n",
      "\n",
      "F1-score: 0.9860\n",
      "\n",
      "Предсказания сохранены в final_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\artificial_intelligence\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Загрузка данных NLTK (однократно)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    return text\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Предобработка данных\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['text'].fillna('').apply(preprocess_text)\n",
    "    df['title'] = df['title'].fillna('').apply(preprocess_text)\n",
    "    df['combined'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Преобразование меток\n",
    "train_df['label'] = train_df['label'].apply(\n",
    "    lambda x: 1 if str(x).strip().lower() in ['1', 'true', 'real', 'yes'] else 0\n",
    ")\n",
    "\n",
    "# Создание пайплайна\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('model', LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1  # Убираем вывод логов\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['combined'],\n",
    "    train_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Оценка\n",
    "valid_pred = pipeline.predict(X_valid)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid, valid_pred))\n",
    "print(f\"\\nF1-score: {f1_score(y_valid, valid_pred, average='weighted'):.4f}\")\n",
    "\n",
    "# Предсказание\n",
    "test_pred = pipeline.predict(test_df['combined'])\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "#results.to_csv('final_predictions.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в final_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65990a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало предобработки текста...\n",
      "Предобработка текста завершена.\n",
      "Добавление новых признаков...\n",
      "Создание пайплайна...\n",
      "Разделение данных...\n",
      "Начало обучения модели...\n",
      "Обучение завершено.\n",
      "Оптимизация порога классификации...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      2223\n",
      "           1       0.99      0.98      0.99      2649\n",
      "\n",
      "    accuracy                           0.98      4872\n",
      "   macro avg       0.98      0.98      0.98      4872\n",
      "weighted avg       0.98      0.98      0.98      4872\n",
      "\n",
      "\n",
      "Best F1-score: 0.9838\n",
      "\n",
      "Кросс-валидация...\n",
      "Cross-validation F1: 0.9841 (±0.0023)\n",
      "\n",
      "Предсказание на тестовых данных...\n",
      "\n",
      "Финальные предсказания сохранены в final_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Решение проблемы с NLTK данными\n",
    "def setup_nltk():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Вызываем функцию настройки NLTK\n",
    "setup_nltk()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Инициализация инструментов NLTK с обработкой ошибок\n",
    "try:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    # Если что-то пошло не так, используем упрощенную обработку\n",
    "    lemmatizer = None\n",
    "    stop_words = set()\n",
    "\n",
    "# Улучшенная функция предобработки текста с защитой от ошибок\n",
    "def enhanced_preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    text = re.sub(r'\\d+', '', text)      # Удаление чисел\n",
    "    \n",
    "    if lemmatizer is not None:\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "            return ' '.join(tokens)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Возвращаем упрощенную версию, если возникли ошибки\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Функция добавления новых признаков\n",
    "def add_features(df):\n",
    "    df['text_len'] = df['text'].apply(len)\n",
    "    df['title_len'] = df['title'].apply(len)\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    df['title_word_count'] = df['title'].apply(lambda x: len(x.split()))\n",
    "    df['word_density'] = df['word_count'] / (df['text_len'] + 1e-6)  # Добавляем малое число для избежания деления на 0\n",
    "    return df\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Предобработка данных\n",
    "print(\"Начало предобработки текста...\")\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['text'].fillna('').apply(enhanced_preprocess)\n",
    "    df['title'] = df['title'].fillna('').apply(enhanced_preprocess)\n",
    "    df['combined'] = df['title'] + ' ' + df['text']\n",
    "print(\"Предобработка текста завершена.\")\n",
    "\n",
    "# Добавление новых признаков\n",
    "print(\"Добавление новых признаков...\")\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "# Преобразование меток\n",
    "train_df['label'] = train_df['label'].apply(\n",
    "    lambda x: 1 if str(x).strip().lower() in ['1', 'true', 'real', 'yes'] else 0\n",
    ")\n",
    "\n",
    "# Создание пайплайна\n",
    "print(\"Создание пайплайна...\")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(\n",
    "            max_features=30000,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            sublinear_tf=True,\n",
    "            analyzer='word'\n",
    "        ), 'combined'),\n",
    "        ('num', StandardScaler(), ['text_len', 'title_len', 'word_count', 'title_word_count', 'word_density'])\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=9,\n",
    "        num_leaves=31,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Разделение данных\n",
    "print(\"Разделение данных...\")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df,\n",
    "    train_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "print(\"Начало обучения модели...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Обучение завершено.\")\n",
    "\n",
    "# Оптимизация порога классификации\n",
    "print(\"Оптимизация порога классификации...\")\n",
    "valid_probs = pipeline.predict_proba(X_valid)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid, valid_probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Оценка модели\n",
    "valid_pred = (valid_probs >= best_threshold).astype(int)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, valid_pred))\n",
    "print(f\"\\nBest F1-score: {f1_score(y_valid, valid_pred, average='weighted'):.4f}\")\n",
    "\n",
    "# Кросс-валидация\n",
    "print(\"\\nКросс-валидация...\")\n",
    "cv_scores = cross_val_score(pipeline, train_df, train_df['label'], \n",
    "                          cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "print(f\"Cross-validation F1: {np.mean(cv_scores):.4f} (±{np.std(cv_scores):.4f})\")\n",
    "\n",
    "# Предсказание на тестовых данных\n",
    "print(\"\\nПредсказание на тестовых данных...\")\n",
    "test_probs = pipeline.predict_proba(test_df)[:, 1]\n",
    "test_pred = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Сохранение результатов\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "#results.to_csv('final_predictions.csv', index=False)\n",
    "print(\"\\nФинальные предсказания сохранены в final_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b46d1578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обучения XGBoost...\n",
      "\n",
      "Обучение завершено за 0.7 минут\n",
      "\n",
      "F1-score на валидации: 0.9839979829970033\n",
      "\n",
      "Предсказания сохранены в final_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Функция очистки текста\n",
    "def clean_text(text):\n",
    "    return str(text).lower().strip()[:2000]  # Ограничение длины текста\n",
    "\n",
    "# Функция очистки меток\n",
    "def clean_label(label):\n",
    "    try:\n",
    "        label = str(label).strip()\n",
    "        if label.lower() in ['0', 'false', 'fake', 'no']:\n",
    "            return 0\n",
    "        elif label.lower() in ['1', 'true', 'real', 'yes']:\n",
    "            return 1\n",
    "        return int(float(label))\n",
    "    except:\n",
    "        return 0  # По умолчанию считаем фейковой новостью\n",
    "\n",
    "# Предобработка данных\n",
    "train_df['text'] = train_df['text'].fillna('').apply(clean_text)\n",
    "train_df['title'] = train_df['title'].fillna('').apply(clean_text)\n",
    "train_df['label'] = train_df['label'].apply(clean_label)\n",
    "test_df['text'] = test_df['text'].fillna('').apply(clean_text)\n",
    "test_df['title'] = test_df['title'].fillna('').apply(clean_text)\n",
    "\n",
    "# Объединение текстовых полей\n",
    "train_df['combined'] = train_df['title'] + \" \" + train_df['text']\n",
    "test_df['combined'] = test_df['title'] + \" \" + test_df['text']\n",
    "\n",
    "# Векторизация текста\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df['combined'])\n",
    "X_test = vectorizer.transform(test_df['combined'])\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "# Разделение на тренировочную и валидационную выборки\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Модель XGBoost с правильными параметрами\n",
    "model = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=None  # Убрали проблемный параметр\n",
    ")\n",
    "\n",
    "# Обучение с ручной проверкой на валидации\n",
    "print(\"Начало обучения XGBoost...\")\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"\\nОбучение завершено за {(time.time() - start_time)/60:.1f} минут\")\n",
    "\n",
    "# Оценка на валидации\n",
    "valid_pred = model.predict(X_valid)\n",
    "print(\"\\nF1-score на валидации:\", f1_score(y_valid, valid_pred, average='weighted'))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание\n",
    "test_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "results.to_csv('predictions3.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в final_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30eaf241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение ансамбля...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      2249\n",
      "           1       0.99      0.98      0.99      2623\n",
      "\n",
      "    accuracy                           0.99      4872\n",
      "   macro avg       0.99      0.99      0.99      4872\n",
      "weighted avg       0.99      0.99      0.99      4872\n",
      "\n",
      "\n",
      "F1-score: 0.9856\n",
      "\n",
      "Предсказания сохранены в ensemble_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    return text\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Предобработка данных\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['text'].fillna('').apply(preprocess_text)\n",
    "    df['title'] = df['title'].fillna('').apply(preprocess_text)\n",
    "    df['combined'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Преобразование меток\n",
    "train_df['label'] = train_df['label'].apply(\n",
    "    lambda x: 1 if str(x).strip().lower() in ['1', 'true', 'real', 'yes'] else 0\n",
    ")\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['combined'],\n",
    "    train_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Инициализация моделей\n",
    "models = {\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        iterations=300,\n",
    "        learning_rate=0.05,\n",
    "        depth=7,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "}\n",
    "\n",
    "# Создание ансамбля\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('LightGBM', models['LightGBM']),\n",
    "        ('XGBoost', models['XGBoost']),\n",
    "        ('CatBoost', models['CatBoost'])\n",
    "    ],\n",
    "    voting='soft',  # Используем soft voting для вероятностей\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Создание пайплайна с ансамблем\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('ensemble', ensemble)\n",
    "])\n",
    "\n",
    "# Обучение\n",
    "print(\"Обучение ансамбля...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Оценка\n",
    "valid_pred = pipeline.predict(X_valid)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_valid, valid_pred))\n",
    "print(f\"\\nF1-score: {f1_score(y_valid, valid_pred, average='weighted'):.4f}\")\n",
    "\n",
    "# Предсказание\n",
    "test_pred = pipeline.predict(test_df['combined'])\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "# results.to_csv('ensemble_predictions.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в ensemble_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb7f2708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Обучение LightGBM ===\n",
      "F1 на train: 0.98216\n",
      "F1 на валидации: 0.98134\n",
      "Время обучения: 35.71 сек\n",
      "Новая лучшая модель: LightGBM\n",
      "\n",
      "=== Обучение XGBoost ===\n",
      "F1 на train: 0.99063\n",
      "F1 на валидации: 0.98513\n",
      "Время обучения: 43.98 сек\n",
      "Новая лучшая модель: XGBoost\n",
      "\n",
      "=== Обучение GradientBoosting ===\n",
      "F1 на train: 0.99521\n",
      "F1 на валидации: 0.98533\n",
      "Время обучения: 182.37 сек\n",
      "Новая лучшая модель: GradientBoosting\n",
      "\n",
      "=== Обучение блендинга ===\n",
      "Лучшие параметры мета-модели: {'C': 100.0, 'class_weight': None, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Блендинг F1 на валидации: 0.98646\n",
      "\n",
      "Примеры ошибок (первые 5):\n",
      "                                                combined  true_label  \\\n",
      "8139   pin drop speech father daughter kidnapped kill...           1   \n",
      "13321  dont believe myth weightlifting slow home mont...           1   \n",
      "10014  thing learned general contractor donald trump ...           1   \n",
      "9296   saoule tout monde avec son analyse politique d...           0   \n",
      "9293   independent tilt decisively trump nevertrumper...           1   \n",
      "\n",
      "       predicted_label  \n",
      "8139                 0  \n",
      "13321                0  \n",
      "10014                0  \n",
      "9296                 1  \n",
      "9293                 0  \n",
      "\n",
      "Статистика ошибок:\n",
      "Всего ошибок: 132\n",
      "Ошибки для класса 0 (фейк): 42\n",
      "Ошибки для класса 1 (реальная): 90\n",
      "\n",
      "=== Кросс-валидация лучшей модели ===\n",
      "Кросс-валидация для блендинга не проводится.\n",
      "\n",
      "=== Предсказание на тестовых данных ===\n",
      "\n",
      "Предсказания сохранены в final_predictions_v2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Инициализация\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fake_keywords = ['shock', 'urgent', 'breaking', 'secret', 'hoax', 'fake', 'scandal', 'conspiracy']\n",
    "political_keywords = ['trump', 'biden', 'election', 'poll', 'democrat', 'republican', 'congress', 'senate']\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z\\s!]', '', text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Функция для подсчёта доли заглавных букв\n",
    "def uppercase_ratio(text):\n",
    "    if not text or len(text) == 0:\n",
    "        return 0.0\n",
    "    letters = [c for c in text if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    return sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "\n",
    "# Функция для подсчёта восклицательных знаков\n",
    "def exclamation_count(text):\n",
    "    return text.count('!')\n",
    "\n",
    "# Функция для доли уникальных слов\n",
    "def unique_word_ratio(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "# Функция для оценки тональности\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Функция для подсчёта политических ключевых слов\n",
    "def political_keyword_count(text):\n",
    "    return sum(text.lower().count(kw) for kw in political_keywords)\n",
    "\n",
    "# Функция очистки меток\n",
    "def clean_label(label):\n",
    "    if pd.isna(label):\n",
    "        return 0\n",
    "    label = str(label).strip().lower()\n",
    "    return 1 if label in ['1', 'true', 'real', 'yes'] else 0\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Предобработка данных\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "    df['title_clean'] = df['title'].apply(preprocess_text)\n",
    "    df['combined'] = df['title_clean'] + ' ' + df['text_clean']\n",
    "    df['text_length'] = df['text_clean'].apply(len)\n",
    "    df['title_length'] = df['title_clean'].apply(len)\n",
    "    df['word_count'] = df['combined'].apply(lambda x: len(x.split()))\n",
    "    df['title_word_count'] = df['title_clean'].apply(lambda x: len(x.split()))\n",
    "    df['fake_keyword_count'] = df['combined'].apply(lambda x: sum(x.lower().count(kw) for kw in fake_keywords))\n",
    "    df['political_keyword_count'] = df['combined'].apply(political_keyword_count)\n",
    "    df['title_uppercase_ratio'] = df['title'].apply(uppercase_ratio)\n",
    "    df['exclamation_count'] = df['title'].apply(exclamation_count)\n",
    "    df['unique_word_ratio'] = df['combined'].apply(unique_word_ratio)\n",
    "    df['title_sentiment'] = df['title'].apply(sentiment_score)\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(clean_label).astype('int8')\n",
    "\n",
    "# Разделение данных\n",
    "X = train_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']]\n",
    "y = train_df['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "# Определение препроцессора\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_tfidf', TfidfVectorizer(max_features=25000, ngram_range=(1, 3), stop_words='english', min_df=2), 'combined'),\n",
    "        ('num', 'passthrough', ['text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment'])\n",
    "    ])\n",
    "\n",
    "# Инициализация моделей\n",
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "class_weights = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "models = {\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            reg_lambda=25.0,\n",
    "            reg_alpha=25.0,\n",
    "            colsample_bytree=0.6,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            class_weight=class_weights\n",
    "        ))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            colsample_bytree=0.6,\n",
    "            reg_lambda=25.0,\n",
    "            random_state=42,\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        ))\n",
    "    ]),\n",
    "    'GradientBoosting': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 1. Оценка индивидуальных моделей\n",
    "best_f1 = 0.0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Обучение {name} ===\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    f1_train = f1_score(y_train, train_pred, average='weighted')\n",
    "    f1_val = f1_score(y_val, val_pred, average='weighted')\n",
    "    print(f\"F1 на train: {f1_train:.5f}\")\n",
    "    print(f\"F1 на валидации: {f1_val:.5f}\")\n",
    "    print(f\"Время обучения: {time.time() - start_time:.2f} сек\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        print(f\"Новая лучшая модель: {name}\")\n",
    "\n",
    "# 2. Блендинг\n",
    "print(\"\\n=== Обучение блендинга ===\")\n",
    "val_preds = np.zeros((X_val.shape[0], len(models)))\n",
    "train_preds = np.zeros((X_train.shape[0], len(models)))\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    val_preds[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "    train_preds[:, i] = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "param_grid_meta = {\n",
    "    'C': [1.0, 10.0, 100.0],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "meta_model = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid_meta, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "meta_model.fit(train_preds, y_train)\n",
    "print(f\"Лучшие параметры мета-модели: {meta_model.best_params_}\")\n",
    "blending_pred = meta_model.predict(val_preds)\n",
    "blending_f1 = f1_score(y_val, blending_pred, average='weighted')\n",
    "print(f\"Блендинг F1 на валидации: {blending_f1:.5f}\")\n",
    "if blending_f1 > best_f1:\n",
    "    best_f1 = blending_f1\n",
    "    best_model = (models, meta_model.best_estimator_)\n",
    "    best_model_name = \"Blending\"\n",
    "\n",
    "# 3. Анализ ошибок\n",
    "if best_model_name == \"Blending\":\n",
    "    errors = X_val[blending_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[blending_pred != y_val]\n",
    "    errors['predicted_label'] = blending_pred[blending_pred != y_val]\n",
    "else:\n",
    "    val_pred = best_model.predict(X_val)\n",
    "    errors = X_val[val_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[val_pred != y_val]\n",
    "    errors['predicted_label'] = val_pred[val_pred != y_val]\n",
    "\n",
    "print(\"\\nПримеры ошибок (первые 5):\")\n",
    "print(errors[['combined', 'true_label', 'predicted_label']].head())\n",
    "print(\"\\nСтатистика ошибок:\")\n",
    "print(f\"Всего ошибок: {len(errors)}\")\n",
    "print(f\"Ошибки для класса 0 (фейк): {len(errors[errors['true_label'] == 0])}\")\n",
    "print(f\"Ошибки для класса 1 (реальная): {len(errors[errors['true_label'] == 1])}\")\n",
    "\n",
    "# 4. Кросс-валидация лучшей модели\n",
    "print(\"\\n=== Кросс-валидация лучшей модели ===\")\n",
    "if best_model_name != \"Blending\":\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='f1_weighted')\n",
    "    print(f\"Cross-Validation F1: {cv_scores.mean():.5f} ± {cv_scores.std():.5f}\")\n",
    "else:\n",
    "    print(\"Кросс-валидация для блендинга не проводится.\")\n",
    "\n",
    "# 5. Предсказание на тестовых данных\n",
    "print(\"\\n=== Предсказание на тестовых данных ===\")\n",
    "if best_model_name == \"Blending\":\n",
    "    test_preds = np.zeros((test_df.shape[0], len(models)))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        test_preds[:, i] = model.predict_proba(test_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']])[:, 1]\n",
    "    test_pred = meta_model.predict(test_preds)\n",
    "else:\n",
    "    test_pred = best_model.predict(test_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']])\n",
    "\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "#results.to_csv('final_predictions_v2.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в final_predictions_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0db3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Обучение LightGBM ===\n",
      "F1 на train: 0.98189\n",
      "F1 на валидации: 0.98155\n",
      "Время обучения: 35.28 сек\n",
      "Кросс-валидация F1: 0.98025 ± 0.00195\n",
      "Новая лучшая модель: LightGBM\n",
      "\n",
      "=== Обучение XGBoost ===\n",
      "F1 на train: 0.99077\n",
      "F1 на валидации: 0.98431\n",
      "Время обучения: 43.93 сек\n",
      "Кросс-валидация F1: 0.98291 ± 0.00207\n",
      "Новая лучшая модель: XGBoost\n",
      "\n",
      "=== Обучение GradientBoosting ===\n",
      "F1 на train: 0.99173\n",
      "F1 на валидации: 0.98492\n",
      "Время обучения: 143.47 сек\n",
      "Кросс-валидация F1: 0.98400 ± 0.00109\n",
      "Новая лучшая модель: GradientBoosting\n",
      "\n",
      "=== Обучение блендинга ===\n",
      "Лучшие параметры мета-модели: {'C': 10.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Блендинг F1 на валидации: 0.98769\n",
      "\n",
      "Примеры ошибок (первые 5):\n",
      "                                                combined  true_label  \\\n",
      "20857  ridiculously stupid thing men keep woman socal...           1   \n",
      "9296   saoule tout monde avec son analyse politique d...           0   \n",
      "9293   independent tilt decisively trump nevertrumper...           1   \n",
      "21905  new country woman minority hit hardest war rum...           1   \n",
      "2073   contaminated food china entering organic label...           0   \n",
      "\n",
      "       predicted_label  \n",
      "20857                0  \n",
      "9296                 1  \n",
      "9293                 0  \n",
      "21905                0  \n",
      "2073                 1  \n",
      "\n",
      "Статистика ошибок:\n",
      "Всего ошибок: 120\n",
      "Ошибки для класса 0 (фейк): 38\n",
      "Ошибки для класса 1 (реальная): 82\n",
      "\n",
      "=== Кросс-валидация лучшей модели ===\n",
      "Кросс-валидация для блендинга не проводится.\n",
      "\n",
      "=== Предсказание на тестовых данных ===\n",
      "\n",
      "Предсказания сохранены в final_predictions_v3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Инициализация\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fake_keywords = ['shock', 'urgent', 'breaking', 'secret', 'hoax', 'fake', 'scandal', 'conspiracy']\n",
    "political_keywords = ['trump', 'biden', 'election', 'poll', 'democrat', 'republican', 'congress', 'senate']\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z\\s!]', '', text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Функция для подсчёта доли заглавных букв\n",
    "def uppercase_ratio(text):\n",
    "    if not text or len(text) == 0:\n",
    "        return 0.0\n",
    "    letters = [c for c in text if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    return sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "\n",
    "# Функция для подсчёта восклицательных знаков\n",
    "def exclamation_count(text):\n",
    "    return text.count('!')\n",
    "\n",
    "# Функция для доли уникальных слов\n",
    "def unique_word_ratio(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "# Функция для оценки тональности\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Функция для подсчёта политических ключевых слов\n",
    "def political_keyword_count(text):\n",
    "    return sum(text.lower().count(kw) for kw in political_keywords)\n",
    "\n",
    "# Функция очистки меток\n",
    "def clean_label(label):\n",
    "    if pd.isna(label):\n",
    "        return 0\n",
    "    label = str(label).strip().lower()\n",
    "    return 1 if label in ['1', 'true', 'real', 'yes'] else 0\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Предобработка данных\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "    df['title_clean'] = df['title'].apply(preprocess_text)\n",
    "    df['combined'] = df['title_clean'] + ' ' + df['text_clean']\n",
    "    df['text_length'] = df['text_clean'].apply(len)\n",
    "    df['title_length'] = df['title_clean'].apply(len)\n",
    "    df['word_count'] = df['combined'].apply(lambda x: len(x.split()))\n",
    "    df['title_word_count'] = df['title_clean'].apply(lambda x: len(x.split()))\n",
    "    df['fake_keyword_count'] = df['combined'].apply(lambda x: sum(x.lower().count(kw) for kw in fake_keywords))\n",
    "    df['political_keyword_count'] = df['combined'].apply(political_keyword_count)\n",
    "    df['title_uppercase_ratio'] = df['title'].apply(uppercase_ratio)\n",
    "    df['exclamation_count'] = df['title'].apply(exclamation_count)\n",
    "    df['unique_word_ratio'] = df['combined'].apply(unique_word_ratio)\n",
    "    df['title_sentiment'] = df['title'].apply(sentiment_score)\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(clean_label).astype('int8')\n",
    "\n",
    "# Разделение данных\n",
    "X = train_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']]\n",
    "y = train_df['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "# Определение препроцессора\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_tfidf', TfidfVectorizer(max_features=25000, ngram_range=(1, 3), stop_words='english', min_df=3), 'combined'),\n",
    "        ('num', 'passthrough', ['text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment'])\n",
    "    ])\n",
    "\n",
    "# Инициализация моделей\n",
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "class_weights = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "models = {\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            reg_lambda=25.0,\n",
    "            reg_alpha=25.0,\n",
    "            colsample_bytree=0.6,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            class_weight=class_weights\n",
    "        ))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            colsample_bytree=0.6,\n",
    "            reg_lambda=25.0,\n",
    "            random_state=42,\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        ))\n",
    "    ]),\n",
    "    'GradientBoosting': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', GradientBoostingClassifier(\n",
    "            n_estimators=150,  # Ускорение\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 1. Оценка индивидуальных моделей с кросс-валидацией\n",
    "best_f1 = 0.0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Обучение {name} ===\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    f1_train = f1_score(y_train, train_pred, average='weighted')\n",
    "    f1_val = f1_score(y_val, val_pred, average='weighted')\n",
    "    print(f\"F1 на train: {f1_train:.5f}\")\n",
    "    print(f\"F1 на валидации: {f1_val:.5f}\")\n",
    "    print(f\"Время обучения: {time.time() - start_time:.2f} сек\")\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "    print(f\"Кросс-валидация F1: {cv_scores.mean():.5f} ± {cv_scores.std():.5f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        print(f\"Новая лучшая модель: {name}\")\n",
    "\n",
    "# 2. Блендинг\n",
    "print(\"\\n=== Обучение блендинга ===\")\n",
    "val_preds = np.zeros((X_val.shape[0], len(models)))\n",
    "train_preds = np.zeros((X_train.shape[0], len(models)))\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    val_preds[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "    train_preds[:, i] = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "param_grid_meta = {\n",
    "    'C': [10.0, 100.0, 200.0],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'solver': ['liblinear'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "meta_model = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid_meta, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "meta_model.fit(train_preds, y_train)\n",
    "print(f\"Лучшие параметры мета-модели: {meta_model.best_params_}\")\n",
    "blending_pred = meta_model.predict(val_preds)\n",
    "blending_f1 = f1_score(y_val, blending_pred, average='weighted')\n",
    "print(f\"Блендинг F1 на валидации: {blending_f1:.5f}\")\n",
    "if blending_f1 > best_f1:\n",
    "    best_f1 = blending_f1\n",
    "    best_model = (models, meta_model.best_estimator_)\n",
    "    best_model_name = \"Blending\"\n",
    "\n",
    "# 3. Анализ ошибок\n",
    "if best_model_name == \"Blending\":\n",
    "    errors = X_val[blending_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[blending_pred != y_val]\n",
    "    errors['predicted_label'] = blending_pred[blending_pred != y_val]\n",
    "else:\n",
    "    val_pred = best_model.predict(X_val)\n",
    "    errors = X_val[val_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[val_pred != y_val]\n",
    "    errors['predicted_label'] = val_pred[val_pred != y_val]\n",
    "\n",
    "print(\"\\nПримеры ошибок (первые 5):\")\n",
    "print(errors[['combined', 'true_label', 'predicted_label']].head())\n",
    "print(\"\\nСтатистика ошибок:\")\n",
    "print(f\"Всего ошибок: {len(errors)}\")\n",
    "print(f\"Ошибки для класса 0 (фейк): {len(errors[errors['true_label'] == 0])}\")\n",
    "print(f\"Ошибки для класса 1 (реальная): {len(errors[errors['true_label'] == 1])}\")\n",
    "\n",
    "# 4. Кросс-валидация лучшей модели\n",
    "print(\"\\n=== Кросс-валидация лучшей модели ===\")\n",
    "if best_model_name != \"Blending\":\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='f1_weighted')\n",
    "    print(f\"Cross-Validation F1: {cv_scores.mean():.5f} ± {cv_scores.std():.5f}\")\n",
    "else:\n",
    "    print(\"Кросс-валидация для блендинга не проводится.\")\n",
    "\n",
    "# 5. Предсказание на тестовых данных\n",
    "print(\"\\n=== Предсказание на тестовых данных ===\")\n",
    "if best_model_name == \"Blending\":\n",
    "    test_preds = np.zeros((test_df.shape[0], len(models)))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        test_preds[:, i] = model.predict_proba(test_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']])[:, 1]\n",
    "    test_pred = meta_model.predict(test_preds)\n",
    "else:\n",
    "    test_pred = best_model.predict(test_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']])\n",
    "\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "results.to_csv('final_predictions_v3.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в final_predictions_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffd9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Обучение LightGBM ===\n",
      "F1 на train: 0.98236\n",
      "F1 на валидации: 0.97949\n",
      "Время обучения: 43.22 сек\n",
      "Новая лучшая модель: LightGBM\n",
      "\n",
      "=== Обучение XGBoost ===\n",
      "F1 на train: 0.98798\n",
      "F1 на валидации: 0.98276\n",
      "Время обучения: 55.99 сек\n",
      "Новая лучшая модель: XGBoost\n",
      "\n",
      "=== Обучение CatBoost ===\n",
      "F1 на train: 0.98757\n",
      "F1 на валидации: 0.98277\n",
      "Время обучения: 67.15 сек\n",
      "Новая лучшая модель: CatBoost\n",
      "\n",
      "=== Обучение блендинга ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Инициализация\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fake_keywords = ['shock', 'urgent', 'breaking', 'secret', 'hoax', 'fake', 'scandal', 'conspiracy']\n",
    "political_keywords = ['trump', 'biden', 'election', 'poll', 'democrat', 'republican', 'congress', 'senate']\n",
    "\n",
    "# Функции предобработки\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z\\s!]', '', text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def uppercase_ratio(text):\n",
    "    if not text or len(text) == 0:\n",
    "        return 0.0\n",
    "    letters = [c for c in text if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    return sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "\n",
    "def exclamation_count(text):\n",
    "    return text.count('!')\n",
    "\n",
    "def emoji_count(text):\n",
    "    return len(emoji.distinct_emoji_list(text))\n",
    "\n",
    "def unique_word_ratio(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def political_keyword_count(text):\n",
    "    return sum(text.lower().count(kw) for kw in political_keywords)\n",
    "\n",
    "def punctuation_count(text):\n",
    "    return sum(1 for c in text if c in '.,!?;:\"')\n",
    "\n",
    "def stop_word_ratio(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return sum(1 for word in words if word.lower() in stop_words) / len(words)\n",
    "\n",
    "def clean_label(label):\n",
    "    if pd.isna(label):\n",
    "        return np.nan\n",
    "    label = str(label).strip().lower()\n",
    "    return 1 if label in ['1', 'true', 'real', 'yes'] else 0\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Очистка данных\n",
    "train_df = train_df.drop(2733, errors='ignore')\n",
    "for df in [train_df, test_df]:\n",
    "    df.drop([f'Unnamed: {i}' for i in range(4, 22)], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# Обработка пропусков\n",
    "train_df['text'] = train_df['text'].fillna('')\n",
    "train_df['title'] = train_df['title'].fillna('')\n",
    "test_df['text'] = test_df['text'].fillna('')\n",
    "test_df['title'] = test_df['title'].fillna('')\n",
    "train_df['label'] = train_df['label'].apply(clean_label)\n",
    "train_df = train_df.dropna(subset=['label'])\n",
    "train_df['label'] = train_df['label'].astype('int8')\n",
    "\n",
    "# Объединение title и text\n",
    "train_df['combined'] = train_df['title'].apply(preprocess_text) + ' ' + train_df['text'].apply(preprocess_text)\n",
    "test_df['combined'] = test_df['title'].apply(preprocess_text) + ' ' + test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Извлечение признаков\n",
    "for df in [train_df, test_df]:\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    df['title_word_count'] = df['title'].apply(lambda x: len(x.split()))\n",
    "    df['uppercase_ratio'] = df['text'].apply(uppercase_ratio)\n",
    "    df['title_uppercase_ratio'] = df['title'].apply(uppercase_ratio)\n",
    "    df['punctuation_count'] = df['combined'].apply(punctuation_count)\n",
    "    df['exclamation_count'] = df['title'].apply(exclamation_count)\n",
    "    df['emoji_count'] = df['title'].apply(emoji_count)\n",
    "    df['stop_word_ratio'] = df['combined'].apply(stop_word_ratio)\n",
    "    df['political_keyword_count'] = df['combined'].apply(political_keyword_count)\n",
    "    df['unique_word_ratio'] = df['combined'].apply(unique_word_ratio)\n",
    "    df['title_sentiment'] = df['title'].apply(sentiment_score)\n",
    "\n",
    "# Признаки\n",
    "feature_cols = ['text_length', 'title_word_count', 'title_uppercase_ratio', 'punctuation_count',\n",
    "                'exclamation_count', 'emoji_count', 'stop_word_ratio', 'political_keyword_count',\n",
    "                'unique_word_ratio', 'title_sentiment']\n",
    "\n",
    "# Разделение данных\n",
    "X = train_df[feature_cols + ['combined']]\n",
    "y = train_df['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Препроцессор\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_tfidf', TfidfVectorizer(max_features=25000, ngram_range=(1, 3), stop_words='english',\n",
    "                                       sublinear_tf=True, min_df=3, max_df=0.9), 'combined'),\n",
    "        ('num', StandardScaler(), feature_cols)\n",
    "    ])\n",
    "\n",
    "# Модели\n",
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "class_weights = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "models = {\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LGBMClassifier(\n",
    "            n_estimators=250,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            reg_lambda=20.0,\n",
    "            reg_alpha=20.0,\n",
    "            colsample_bytree=0.5,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            class_weight=class_weights\n",
    "        ))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(\n",
    "            n_estimators=250,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            colsample_bytree=0.5,\n",
    "            reg_lambda=20.0,\n",
    "            random_state=42,\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        ))\n",
    "    ]),\n",
    "    'CatBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', CatBoostClassifier(\n",
    "            iterations=250,\n",
    "            learning_rate=0.05,\n",
    "            depth=3,\n",
    "            random_state=42,\n",
    "            verbose=0,\n",
    "            auto_class_weights='Balanced'\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Оценка моделей\n",
    "best_f1 = 0.0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Обучение {name} ===\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    f1_train = f1_score(y_train, train_pred, average='weighted')\n",
    "    f1_val = f1_score(y_val, val_pred, average='weighted')\n",
    "    print(f\"F1 на train: {f1_train:.5f}\")\n",
    "    print(f\"F1 на валидации: {f1_val:.5f}\")\n",
    "    print(f\"Время обучения: {time.time() - start_time:.2f} сек\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        print(f\"Новая лучшая модель: {name}\")\n",
    "\n",
    "# Блендинг\n",
    "print(\"\\n=== Обучение блендинга ===\")\n",
    "val_preds = np.zeros((X_val.shape[0], len(models)))\n",
    "train_preds = np.zeros((X_train.shape[0], len(models)))\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    val_preds[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "    train_preds[:, i] = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "param_grid_meta = {\n",
    "    'iterations': [150, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3]\n",
    "}\n",
    "meta_model = GridSearchCV(CatBoostClassifier(random_state=42, verbose=0), param_grid_meta, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "meta_model.fit(train_preds, y_train)\n",
    "print(f\"Лучшие параметры мета-модели: {meta_model.best_params_}\")\n",
    "\n",
    "# Взвешивание предсказаний\n",
    "weights = [0.4, 0.3, 0.3]  # CatBoost, XGBoost, LightGBM\n",
    "blending_pred_proba = np.sum([val_preds[:, i] * w for i, w in enumerate(weights)], axis=0)\n",
    "blending_pred = (blending_pred_proba >= 0.5).astype(int)\n",
    "blending_f1 = f1_score(y_val, blending_pred, average='weighted')\n",
    "print(f\"Блендинг F1 на валидации (взвешенный): {blending_f1:.5f}\")\n",
    "\n",
    "if blending_f1 > best_f1:\n",
    "    best_f1 = blending_f1\n",
    "    best_model = (models, meta_model.best_estimator_, weights)\n",
    "    best_model_name = \"Blending\"\n",
    "\n",
    "# Анализ ошибок\n",
    "if best_model_name == \"Blending\":\n",
    "    errors = X_val[blending_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[blending_pred != y_val]\n",
    "    errors['predicted_label'] = blending_pred[blending_pred != y_val]\n",
    "else:\n",
    "    val_pred = best_model.predict(X_val)\n",
    "    errors = X_val[val_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[val_pred != y_val]\n",
    "    errors['predicted_label'] = val_pred[val_pred != y_val]\n",
    "\n",
    "print(\"\\nПримеры ошибок (первые 5):\")\n",
    "print(errors[['combined', 'true_label', 'predicted_label']].head())\n",
    "print(\"\\nСтатистика ошибок:\")\n",
    "print(f\"Всего ошибок: {len(errors)}\")\n",
    "print(f\"Ошибки для класса 0 (фейк): {len(errors[errors['true_label'] == 0])}\")\n",
    "print(f\"Ошибки для класса 1 (реальная): {len(errors[errors['true_label'] == 1])}\")\n",
    "\n",
    "# Финальное обучение\n",
    "print(\"\\n=== Финальное обучение ===\")\n",
    "if best_model_name == \"Blending\":\n",
    "    for model in models.values():\n",
    "        model.fit(X, y)\n",
    "    train_preds_full = np.zeros((X.shape[0], len(models)))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        train_preds_full[:, i] = model.predict_proba(X)[:, 1]\n",
    "    meta_model.best_estimator_.fit(train_preds_full, y)\n",
    "else:\n",
    "    best_model.fit(X, y)\n",
    "\n",
    "# Предсказание\n",
    "X_test = test_df[feature_cols + ['combined']]\n",
    "if best_model_name == \"Blending\":\n",
    "    test_preds = np.zeros((X_test.shape[0], len(models)))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        test_preds[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "    test_pred_proba = np.sum([test_preds[:, i] * w for i, w in enumerate(weights)], axis=0)\n",
    "    test_pred = (test_pred_proba >= 0.5).astype(int)\n",
    "else:\n",
    "    test_pred = best_model.predict(X_test)\n",
    "\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "results.to_csv('submission_v9.csv', index=False)\n",
    "print(\"\\nПредсказания сохранены в submission_v9.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff00efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Загрузка данных NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Инициализация\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fake_keywords = ['shock', 'urgent', 'breaking', 'secret', 'hoax', 'fake', 'scandal', 'conspiracy']\n",
    "political_keywords = ['trump', 'biden', 'election', 'poll', 'democrat', 'republican', 'congress', 'senate']\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z\\s!]', '', text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Функция для подсчёта доли заглавных букв\n",
    "def uppercase_ratio(text):\n",
    "    if not text or len(text) == 0:\n",
    "        return 0.0\n",
    "    letters = [c for c in text if c.isalpha()]\n",
    "    if not letters:\n",
    "        return 0.0\n",
    "    return sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "\n",
    "# Функция для подсчёта восклицательных знаков\n",
    "def exclamation_count(text):\n",
    "    return text.count('!')\n",
    "\n",
    "# Функция для доли уникальных слов\n",
    "def unique_word_ratio(text):\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "# Функция для оценки тональности\n",
    "def sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Функция для подсчёта политических ключевых слов\n",
    "def political_keyword_count(text):\n",
    "    return sum(text.lower().count(kw) for kw in political_keywords)\n",
    "\n",
    "# Функция очистки меток\n",
    "def clean_label(label):\n",
    "    if pd.isna(label):\n",
    "        return 0\n",
    "    label = str(label).strip().lower()\n",
    "    return 1 if label in ['1', 'true', 'real', 'yes'] else 0\n",
    "\n",
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Предобработка данных\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "    df['title_clean'] = df['title'].apply(preprocess_text)\n",
    "    df['combined'] = df['title_clean'] + ' ' + df['text_clean']\n",
    "    df['text_length'] = df['text_clean'].apply(len)\n",
    "    df['title_length'] = df['title_clean'].apply(len)\n",
    "    df['word_count'] = df['combined'].apply(lambda x: len(x.split()))\n",
    "    df['title_word_count'] = df['title_clean'].apply(lambda x: len(x.split()))\n",
    "    df['fake_keyword_count'] = df['combined'].apply(lambda x: sum(x.lower().count(kw) for kw in fake_keywords))\n",
    "    df['political_keyword_count'] = df['combined'].apply(political_keyword_count)\n",
    "    df['title_uppercase_ratio'] = df['title'].apply(uppercase_ratio)\n",
    "    df['exclamation_count'] = df['title'].apply(exclamation_count)\n",
    "    df['unique_word_ratio'] = df['combined'].apply(unique_word_ratio)\n",
    "    df['title_sentiment'] = df['title'].apply(sentiment_score)\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(clean_label).astype('int8')\n",
    "\n",
    "# Разделение данных\n",
    "X = train_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']]\n",
    "y = train_df['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "# Определение препроцессора\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_tfidf', TfidfVectorizer(max_features=25000, ngram_range=(1, 3), stop_words='english', min_df=3), 'combined'),\n",
    "        ('num', 'passthrough', ['text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment'])\n",
    "    ])\n",
    "\n",
    "# Инициализация моделей\n",
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "class_weights = {0: 1.0, 1: scale_pos_weight}\n",
    "\n",
    "models = {\n",
    "    'LightGBM': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            reg_lambda=25.0,\n",
    "            reg_alpha=25.0,\n",
    "            colsample_bytree=0.6,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            class_weight=class_weights\n",
    "        ))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            colsample_bytree=0.6,\n",
    "            reg_lambda=25.0,\n",
    "            random_state=42,\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        ))\n",
    "    ]),\n",
    "    'GradientBoosting': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', GradientBoostingClassifier(\n",
    "            n_estimators=150,  # Ускорение\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 1. Оценка индивидуальных моделей с кросс-валидацией\n",
    "best_f1 = 0.0\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Обучение {name} ===\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    f1_train = f1_score(y_train, train_pred, average='weighted')\n",
    "    f1_val = f1_score(y_val, val_pred, average='weighted')\n",
    "    print(f\"F1 на train: {f1_train:.5f}\")\n",
    "    print(f\"F1 на валидации: {f1_val:.5f}\")\n",
    "    print(f\"Время обучения: {time.time() - start_time:.2f} сек\")\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "    print(f\"Кросс-валидация F1: {cv_scores.mean():.5f} ± {cv_scores.std():.5f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        print(f\"Новая лучшая модель: {name}\")\n",
    "\n",
    "# 2. Блендинг\n",
    "print(\"\\n=== Обучение блендинга ===\")\n",
    "val_preds = np.zeros((X_val.shape[0], len(models)))\n",
    "train_preds = np.zeros((X_train.shape[0], len(models)))\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    val_preds[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "    train_preds[:, i] = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "param_grid_meta = {\n",
    "    'C': [10.0, 100.0, 200.0],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'solver': ['liblinear'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "meta_model = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid_meta, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "meta_model.fit(train_preds, y_train)\n",
    "print(f\"Лучшие параметры мета-модели: {meta_model.best_params_}\")\n",
    "blending_pred = meta_model.predict(val_preds)\n",
    "blending_f1 = f1_score(y_val, blending_pred, average='weighted')\n",
    "print(f\"Блендинг F1 на валидации: {blending_f1:.5f}\")\n",
    "if blending_f1 > best_f1:\n",
    "    best_f1 = blending_f1\n",
    "    best_model = (models, meta_model.best_estimator_)\n",
    "    best_model_name = \"Blending\"\n",
    "\n",
    "# 3. Анализ ошибок\n",
    "if best_model_name == \"Blending\":\n",
    "    errors = X_val[blending_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[blending_pred != y_val]\n",
    "    errors['predicted_label'] = blending_pred[blending_pred != y_val]\n",
    "else:\n",
    "    val_pred = best_model.predict(X_val)\n",
    "    errors = X_val[val_pred != y_val].copy()\n",
    "    errors['true_label'] = y_val[val_pred != y_val]\n",
    "    errors['predicted_label'] = val_pred[val_pred != y_val]\n",
    "\n",
    "print(\"\\nПримеры ошибок (первые 5):\")\n",
    "print(errors[['combined', 'true_label', 'predicted_label']].head())\n",
    "print(\"\\nСтатистика ошибок:\")\n",
    "print(f\"Всего ошибок: {len(errors)}\")\n",
    "print(f\"Ошибки для класса 0 (фейк): {len(errors[errors['true_label'] == 0])}\")\n",
    "print(f\"Ошибки для класса 1 (реальная): {len(errors[errors['true_label'] == 1])}\")\n",
    "\n",
    "# 4. Кросс-валидация лучшей модели\n",
    "print(\"\\nКросс-валидация лучшей модели:\")\n",
    "if best_model_name != \"Blending\":\n",
    "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='f1_weighted')\n",
    "    print(f\"Cross-Validation F1: {cv_scores.mean():.5f} ± {cv_scores.std():.5f}\")\n",
    "else:\n",
    "    print(\"Кросс-валидация для блендинга не проводится.\")\n",
    "\n",
    "# 5. Предсказание на тестовых данных\n",
    "print(\"\\nПредсказание на тестовых данных:\")\n",
    "if best_model_name == \"Blending\":\n",
    "    test_preds = np.zeros((test_df.shape[0], len(models)))\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        model.fit(X_train, y_train)\n",
    "        test_preds[:, i] = model.predict_proba(test_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']])[:, 1]\n",
    "    test_pred = meta_model.predict(test_preds)\n",
    "else:\n",
    "    test_pred = best_model.predict(test_df[['combined', 'text_length', 'title_length', 'word_count', 'title_word_count', 'fake_keyword_count', 'political_keyword_count', 'title_uppercase_ratio', 'exclamation_count', 'unique_word_ratio', 'title_sentiment']])\n",
    "\n",
    "results = pd.DataFrame({'id': test_df['id'], 'label': test_pred})\n",
    "results.to_csv('predictions_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
